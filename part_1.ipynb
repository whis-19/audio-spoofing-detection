{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7844c1b",
   "metadata": {},
   "source": [
    "# Deepfake Audio Detection Using MFCC and Classical + Deep Learning Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2052507d",
   "metadata": {},
   "source": [
    "\n",
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d4470d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (6794, 2600)\n",
      "Label distribution: [3398 3396]\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - accuracy: 0.7700 - loss: 5.8392 - val_accuracy: 0.9375 - val_loss: 2.6463\n",
      "Epoch 2/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9444 - loss: 2.1146 - val_accuracy: 0.9467 - val_loss: 1.0654\n",
      "Epoch 3/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9604 - loss: 0.9048 - val_accuracy: 0.9540 - val_loss: 0.6762\n",
      "Epoch 4/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9537 - loss: 0.6278 - val_accuracy: 0.9522 - val_loss: 0.5304\n",
      "Epoch 5/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9548 - loss: 0.5204 - val_accuracy: 0.9522 - val_loss: 0.4502\n",
      "Epoch 6/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9446 - loss: 0.4534 - val_accuracy: 0.9485 - val_loss: 0.4483\n",
      "Epoch 7/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9601 - loss: 0.3945 - val_accuracy: 0.9430 - val_loss: 0.4253\n",
      "Epoch 8/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9534 - loss: 0.4018 - val_accuracy: 0.9540 - val_loss: 0.4077\n",
      "Epoch 9/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9514 - loss: 0.4082 - val_accuracy: 0.9412 - val_loss: 0.4025\n",
      "Epoch 10/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9583 - loss: 0.3742 - val_accuracy: 0.9357 - val_loss: 0.4369\n",
      "Epoch 11/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9547 - loss: 0.4026 - val_accuracy: 0.9504 - val_loss: 0.3726\n",
      "Epoch 12/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 12ms/step - accuracy: 0.9545 - loss: 0.3750 - val_accuracy: 0.9540 - val_loss: 0.3979\n",
      "Epoch 13/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9640 - loss: 0.3541 - val_accuracy: 0.9283 - val_loss: 0.4857\n",
      "Epoch 14/20\n",
      "\u001b[1m153/153\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - accuracy: 0.9583 - loss: 0.3802 - val_accuracy: 0.9559 - val_loss: 0.4123\n",
      "\u001b[1m43/43\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Model saved to binary-models\\logistic_regression_model.pkl"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model saved to binary-models\\svm_model.pkl\n",
      "Model saved to binary-models\\perceptron_model.pkl\n",
      "Model saved to binary-models\\perceptron_online_model.pkl\n",
      "Tuned DNN model saved to binary-models\\dnn_model.h5\n",
      "\n",
      "=== Tuned Logistic Regression ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       680\n",
      "           1       0.90      0.90      0.90       679\n",
      "\n",
      "    accuracy                           0.90      1359\n",
      "   macro avg       0.90      0.90      0.90      1359\n",
      "weighted avg       0.90      0.90      0.90      1359\n",
      "\n",
      "AUC-ROC: 0.9663475699558173\n",
      "\n",
      "=== Tuned SVM ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97       680\n",
      "           1       0.96      0.97      0.97       679\n",
      "\n",
      "    accuracy                           0.97      1359\n",
      "   macro avg       0.97      0.97      0.97      1359\n",
      "weighted avg       0.97      0.97      0.97      1359\n",
      "\n",
      "AUC-ROC: 0.9947013341419042\n",
      "\n",
      "=== Tuned Perceptron ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       680\n",
      "           1       0.90      0.90      0.90       679\n",
      "\n",
      "    accuracy                           0.90      1359\n",
      "   macro avg       0.90      0.90      0.90      1359\n",
      "weighted avg       0.90      0.90      0.90      1359\n",
      "\n",
      "\n",
      "=== Tuned Online Perceptron ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.86      0.86       680\n",
      "           1       0.86      0.87      0.86       679\n",
      "\n",
      "    accuracy                           0.86      1359\n",
      "   macro avg       0.86      0.86      0.86      1359\n",
      "weighted avg       0.86      0.86      0.86      1359\n",
      "\n",
      "\n",
      "=== Tuned DNN ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95       680\n",
      "           1       0.96      0.94      0.95       679\n",
      "\n",
      "    accuracy                           0.95      1359\n",
      "   macro avg       0.95      0.95      0.95      1359\n",
      "weighted avg       0.95      0.95      0.95      1359\n",
      "\n",
      "AUC-ROC: 0.9882179675994109\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "import joblib\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Optional: suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_features_recursive(root_dir, label_value, sr=16000, n_mfcc=13, max_len=200):\n",
    "    features, labels = [], []\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".wav\"):\n",
    "                try:\n",
    "                    file_path = os.path.join(dirpath, file)\n",
    "                    signal, _ = librosa.load(file_path, sr=sr)\n",
    "                    mfcc = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=n_mfcc)\n",
    "                    if mfcc.shape[1] < max_len:\n",
    "                        pad_width = max_len - mfcc.shape[1]\n",
    "                        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "                    else:\n",
    "                        mfcc = mfcc[:, :max_len]\n",
    "                    features.append(mfcc.flatten())\n",
    "                    labels.append(label_value)\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to process {file_path}: {e}\")\n",
    "    return features, labels\n",
    "\n",
    "base_dir = r\"./deepfake_detection_dataset_urdu\"\n",
    "bonafide_feat, bonafide_lbl = extract_features_recursive(os.path.join(base_dir, \"Bonafide\"), 0)\n",
    "tacotron_feat, tacotron_lbl = extract_features_recursive(os.path.join(base_dir, \"Spoofed_Tacotron\"), 1)\n",
    "vits_feat, vits_lbl = extract_features_recursive(os.path.join(base_dir, \"Spoofed_TTS\"), 1)\n",
    "\n",
    "X = np.array(bonafide_feat + tacotron_feat + vits_feat)\n",
    "y = np.array(bonafide_lbl + tacotron_lbl + vits_lbl)\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Label distribution:\", np.bincount(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Logistic Regression Hyperparameter Tuning with RandomizedSearchCV\n",
    "lr_pipeline = Pipeline([\n",
    "    ('clf', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "lr_params = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__penalty': ['l2'],\n",
    "    'clf__solver': ['liblinear', 'saga']\n",
    "}\n",
    "\n",
    "lr_random_search = RandomizedSearchCV(lr_pipeline, param_distributions=lr_params, n_iter=10, scoring='roc_auc', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "lr_random_search.fit(X_train_scaled, y_train)\n",
    "best_lr = lr_random_search.best_estimator_\n",
    "\n",
    "# SVM Hyperparameter Tuning with RandomizedSearchCV\n",
    "svm_pipeline = Pipeline([\n",
    "    ('clf', SVC(probability=True))\n",
    "])\n",
    "\n",
    "svm_params = {\n",
    "    'clf__C': [0.1, 1, 10],\n",
    "    'clf__kernel': ['linear', 'rbf'],\n",
    "    'clf__gamma': ['scale', 'auto']\n",
    "}\n",
    "\n",
    "svm_random_search = RandomizedSearchCV(svm_pipeline, param_distributions=svm_params, n_iter=10, scoring='roc_auc', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "svm_random_search.fit(X_train_scaled, y_train)\n",
    "best_svm = svm_random_search.best_estimator_\n",
    "\n",
    "# Perceptron Hyperparameter Tuning with RandomizedSearchCV\n",
    "perc_pipeline = Pipeline([\n",
    "    ('clf', Perceptron(max_iter=1000))\n",
    "])\n",
    "\n",
    "perc_params = {\n",
    "    'clf__eta0': [0.01, 0.1, 0.5, 1],\n",
    "    'clf__penalty': [None, 'l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "perc_random_search = RandomizedSearchCV(perc_pipeline, param_distributions=perc_params, n_iter=10, scoring='roc_auc', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "perc_random_search.fit(X_train_scaled, y_train)\n",
    "best_perc = perc_random_search.best_estimator_\n",
    "\n",
    "# Online Perceptron Hyperparameter Tuning with RandomizedSearchCV\n",
    "online_pipeline = Pipeline([\n",
    "    ('clf', Perceptron(max_iter=1, warm_start=True, eta0=1.0))\n",
    "])\n",
    "\n",
    "online_params = {\n",
    "    'clf__eta0': [0.01, 0.1, 1],\n",
    "    'clf__penalty': [None, 'l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "online_random_search = RandomizedSearchCV(online_pipeline, param_distributions=online_params, n_iter=10, scoring='roc_auc', cv=3, verbose=1, random_state=42, n_jobs=-1)\n",
    "online_random_search.fit(X_train_scaled, y_train)\n",
    "best_online_model = online_random_search.best_estimator_\n",
    "\n",
    "# DNN\n",
    "dnn = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train_scaled.shape[1],), kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "    Dropout(0.4),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "dnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "dnn.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    epochs=20, batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_prob_dnn = dnn.predict(X_test_scaled).flatten()\n",
    "y_pred_dnn = (y_prob_dnn > 0.5).astype(int)\n",
    "\n",
    "# SAVE MODELs\n",
    "def save_model(model, filename):\n",
    "    try:\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save model {filename}: {e}\")\n",
    "\n",
    "output_dir = \"binary-models\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "save_model(best_lr, os.path.join(output_dir, \"logistic_regression_model.pkl\"))\n",
    "save_model(best_svm, os.path.join(output_dir, \"svm_model.pkl\"))\n",
    "save_model(best_perc, os.path.join(output_dir, \"perceptron_model.pkl\"))\n",
    "save_model(best_online_model, os.path.join(output_dir, \"perceptron_online_model.pkl\"))\n",
    "\n",
    "try:\n",
    "    dnn.save(os.path.join(output_dir, \"dnn_model.h5\"))\n",
    "    print(f\"Tuned DNN model saved to {os.path.join(output_dir, 'dnn_model.h5')}\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to save DNN model: {e}\")\n",
    "\n",
    "# EVALUATION\n",
    "def print_results(name, y_true, y_pred, y_prob=None):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    if y_prob is not None:\n",
    "        print(\"AUC-ROC:\", roc_auc_score(y_true, y_prob))\n",
    "\n",
    "print_results(\"Tuned Logistic Regression\", y_test, best_lr.predict(X_test_scaled), best_lr.predict_proba(X_test_scaled)[:, 1])\n",
    "print_results(\"Tuned SVM\", y_test, best_svm.predict(X_test_scaled), best_svm.predict_proba(X_test_scaled)[:, 1])\n",
    "print_results(\"Tuned Perceptron\", y_test, best_perc.predict(X_test_scaled))\n",
    "print_results(\"Tuned Online Perceptron\", y_test, best_online_model.predict(X_test_scaled))\n",
    "print_results(\"Tuned DNN\", y_test, y_pred_dnn, y_prob_dnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
