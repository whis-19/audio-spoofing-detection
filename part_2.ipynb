{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c022c479",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c07b0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n",
      "Epoch 1/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 21ms/step - accuracy: 0.3481 - loss: 4.8444 - val_accuracy: 0.4615 - val_loss: 1.4281\n",
      "Epoch 2/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4373 - loss: 1.2208 - val_accuracy: 0.4615 - val_loss: 0.8028\n",
      "Epoch 3/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - accuracy: 0.4177 - loss: 0.7255 - val_accuracy: 0.4615 - val_loss: 0.5770\n",
      "Epoch 4/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4181 - loss: 0.5434 - val_accuracy: 0.4615 - val_loss: 0.4927\n",
      "Epoch 5/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4431 - loss: 0.4764 - val_accuracy: 0.4615 - val_loss: 0.4620\n",
      "Epoch 6/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4177 - loss: 0.4563 - val_accuracy: 0.4615 - val_loss: 0.4498\n",
      "Epoch 7/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4426 - loss: 0.4384 - val_accuracy: 0.4615 - val_loss: 0.4469\n",
      "Epoch 8/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.4211 - loss: 0.4423 - val_accuracy: 0.4615 - val_loss: 0.4448\n",
      "Epoch 9/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4346 - loss: 0.4463 - val_accuracy: 0.4615 - val_loss: 0.4453\n",
      "Epoch 10/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4551 - loss: 0.4342 - val_accuracy: 0.4615 - val_loss: 0.4454\n",
      "Epoch 11/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.3881 - loss: 0.4513 - val_accuracy: 0.4615 - val_loss: 0.4435\n",
      "Epoch 12/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4328 - loss: 0.4454 - val_accuracy: 0.4615 - val_loss: 0.4445\n",
      "Epoch 13/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4162 - loss: 0.4384 - val_accuracy: 0.4615 - val_loss: 0.4423\n",
      "Epoch 14/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.4350 - loss: 0.4286 - val_accuracy: 0.4615 - val_loss: 0.4423\n",
      "Epoch 15/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4118 - loss: 0.4372 - val_accuracy: 0.4615 - val_loss: 0.4417\n",
      "Epoch 16/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.4204 - loss: 0.4304 - val_accuracy: 0.4615 - val_loss: 0.4420\n",
      "Epoch 17/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4130 - loss: 0.4350 - val_accuracy: 0.4615 - val_loss: 0.4427\n",
      "Epoch 18/20\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.4317 - loss: 0.4405 - val_accuracy: 0.4615 - val_loss: 0.4421\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to multi-label-classifier\\logistic_regression_model.pkl\n",
      "Model saved to multi-label-classifier\\svm_model.pkl\n",
      "Model saved to multi-label-classifier\\perceptron_batch_model.pkl\n",
      "Model saved to multi-label-classifier\\perceptron_online_model.pkl\n",
      "Tuned DNN model saved to multi-label-classifier\\dnn_model.h5\n",
      "\n",
      "=== Model Performance Summary ===\n",
      "                 Model  Hamming Loss  Micro-F1  Macro-F1  Performance@k\n",
      "0  Logistic Regression      0.137019  0.788104  0.457499       0.813299\n",
      "1                  SVM      0.127404  0.804428  0.585917       0.823678\n",
      "2   Perceptron (Batch)      0.141827  0.787515  0.653086       0.786571\n",
      "3  Perceptron (Online)      0.154647  0.767750  0.655426       0.768675\n",
      "4                  DNN      0.229167  0.541667  0.149425       0.812500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression, Perceptron\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import hamming_loss, f1_score, make_scorer, precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import warnings\n",
    "import joblib\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "# Optional: suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df.drop(columns=['type_task'], inplace=True)\n",
    "\n",
    "# Split features and labels\n",
    "X_text = df['report']\n",
    "Y = df.drop(columns=['report'])\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "X = tfidf.fit_transform(X_text)\n",
    "\n",
    "# Train/Validation/Test split\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.1765, random_state=42)\n",
    "\n",
    "# Logistic Regression\n",
    "lr_model = OneVsRestClassifier(LogisticRegression(max_iter=1000))\n",
    "lr_model.fit(X_train, Y_train)\n",
    "Y_val_pred_lr = lr_model.predict(X_val)\n",
    "\n",
    "# SVM\n",
    "svm_model = OneVsRestClassifier(LinearSVC())\n",
    "svm_model.fit(X_train, Y_train)\n",
    "Y_val_pred_svm = svm_model.predict(X_val)\n",
    "\n",
    "# Perceptron (Batch)\n",
    "perc_model = OneVsRestClassifier(Perceptron(max_iter=1000, eta0=1.0, random_state=42))\n",
    "perc_model.fit(X_train, Y_train)\n",
    "Y_val_pred_perc = perc_model.predict(X_val)\n",
    "\n",
    "# Perceptron (Online Learning)\n",
    "classes = [np.array([0, 1])] * Y_train.shape[1]\n",
    "online_model = OneVsRestClassifier(Perceptron(max_iter=1, warm_start=True, eta0=1.0))\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_sample = X_train[i]\n",
    "    Y_sample = Y_train.iloc[i:i+1]\n",
    "    if i == 0:\n",
    "        online_model.estimators_ = []\n",
    "        for j in range(Y_train.shape[1]):\n",
    "            est = Perceptron(max_iter=1, warm_start=True)\n",
    "            est.partial_fit(X_sample, Y_sample.iloc[:, j], classes=[0, 1])\n",
    "            online_model.estimators_.append(est)\n",
    "    else:\n",
    "        for j, est in enumerate(online_model.estimators_):\n",
    "            est.partial_fit(X_sample, Y_sample.iloc[:, j], classes=[0, 1])\n",
    "Y_val_pred_online = np.array([est.predict(X_val) for est in online_model.estimators_]).T\n",
    "\n",
    "# Hyperparameter tuning for Logistic Regression\n",
    "f1_micro_scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "lr_pipeline = Pipeline([('clf', OneVsRestClassifier(LogisticRegression(max_iter=1000)))])\n",
    "lr_params = {\n",
    "    'clf__estimator__C': [0.1, 1, 10],\n",
    "    'clf__estimator__penalty': ['l2']\n",
    "}\n",
    "lr_grid = GridSearchCV(lr_pipeline, param_grid=lr_params, scoring=f1_micro_scorer, cv=3, verbose=1)\n",
    "lr_grid.fit(X_train, Y_train)\n",
    "best_lr_model = lr_grid.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for SVM\n",
    "svm_pipeline = Pipeline([('clf', OneVsRestClassifier(LinearSVC()))])\n",
    "svm_params = {\n",
    "    'clf__estimator__C': [0.1, 1, 10],\n",
    "    'clf__estimator__loss': ['hinge', 'squared_hinge']\n",
    "}\n",
    "svm_grid = GridSearchCV(svm_pipeline, param_grid=svm_params, scoring=f1_micro_scorer, cv=3, verbose=1)\n",
    "svm_grid.fit(X_train, Y_train)\n",
    "best_svm_model = svm_grid.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Batch Perceptron\n",
    "perc_pipeline = Pipeline([('clf', OneVsRestClassifier(Perceptron(max_iter=1000, random_state=42)))])\n",
    "perc_params = {\n",
    "    'clf__estimator__eta0': [0.01, 0.1, 1],\n",
    "    'clf__estimator__penalty': [None, 'l2', 'l1', 'elasticnet']\n",
    "}\n",
    "perc_grid = GridSearchCV(perc_pipeline, param_grid=perc_params, scoring=f1_micro_scorer, cv=3, verbose=1)\n",
    "perc_grid.fit(X_train, Y_train)\n",
    "best_perc_model = perc_grid.best_estimator_\n",
    "\n",
    "# Hyperparameter tuning for Online Perceptron\n",
    "online_pipeline = Pipeline([('clf', OneVsRestClassifier(Perceptron(max_iter=1, warm_start=True, eta0=1.0)))])\n",
    "online_params = {\n",
    "    'clf__estimator__eta0': [0.01, 0.1, 1],\n",
    "    'clf__estimator__penalty': [None, 'l2', 'l1', 'elasticnet']\n",
    "}\n",
    "online_grid = GridSearchCV(online_pipeline, param_grid=online_params, scoring=f1_micro_scorer, cv=3, verbose=1)\n",
    "online_grid.fit(X_train, Y_train)\n",
    "best_online_model = online_grid.best_estimator_\n",
    "\n",
    "# Deep Neural Network\n",
    "try:\n",
    "    X_train_dense = X_train.toarray()\n",
    "    X_val_dense = X_val.toarray()\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(256, activation='relu', input_shape=(X_train_dense.shape[1],), kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.4),\n",
    "        Dense(128, activation='relu', kernel_regularizer=l2(0.01)),\n",
    "        Dropout(0.4),\n",
    "        Dense(Y_train.shape[1], activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    model.fit(X_train_dense, Y_train, epochs=20, batch_size=32, validation_data=(X_val_dense, Y_val), callbacks=[early_stopping], verbose=1)\n",
    "    Y_val_pred_dnn = (model.predict(X_val_dense) > 0.5).astype(int)\n",
    "\n",
    "    dnn_metrics = {\n",
    "        \"Hamming Loss\": hamming_loss(Y_val, Y_val_pred_dnn),\n",
    "        \"Micro-F1\": f1_score(Y_val, Y_val_pred_dnn, average='micro'),\n",
    "        \"Macro-F1\": f1_score(Y_val, Y_val_pred_dnn, average='macro'),\n",
    "        \"Precision@k\": precision_score(Y_val, Y_val_pred_dnn, average='micro', zero_division=0)\n",
    "    }\n",
    "except ImportError:\n",
    "    dnn_metrics = {\"Hamming Loss\": None, \"Micro-F1\": None, \"Macro-F1\": None}\n",
    "    print(\"TensorFlow not installed. Skipping DNN model.\")\n",
    "\n",
    "# Ensure the folder exists\n",
    "output_folder = \"multi-label-classifier\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "def save_model(model, filename):\n",
    "    try:\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save model {filename}: {e}\")\n",
    "\n",
    "save_model(best_lr_model, os.path.join(output_folder, \"logistic_regression_model.pkl\"))\n",
    "save_model(best_svm_model, os.path.join(output_folder, \"svm_model.pkl\"))\n",
    "save_model(best_perc_model, os.path.join(output_folder, \"perceptron_batch_model.pkl\"))\n",
    "save_model(best_online_model, os.path.join(output_folder, \"perceptron_online_model.pkl\"))\n",
    "\n",
    "if 'model' in locals():\n",
    "    model.save(os.path.join(output_folder, \"dnn_model.h5\"))\n",
    "    print(f\"Tuned DNN model saved to {os.path.join(output_folder, 'dnn_model.h5')}\")\n",
    "\n",
    "# Compile results\n",
    "results = {\n",
    "    \"Model\": [\"Logistic Regression\", \"SVM\", \"Perceptron (Batch)\", \"Perceptron (Online)\", \"DNN\"],\n",
    "    \"Hamming Loss\": [\n",
    "        hamming_loss(Y_val, Y_val_pred_lr),\n",
    "        hamming_loss(Y_val, Y_val_pred_svm),\n",
    "        hamming_loss(Y_val, Y_val_pred_perc),\n",
    "        hamming_loss(Y_val, Y_val_pred_online),\n",
    "        dnn_metrics[\"Hamming Loss\"]\n",
    "    ],\n",
    "    \"Micro-F1\": [\n",
    "        f1_score(Y_val, Y_val_pred_lr, average='micro'),\n",
    "        f1_score(Y_val, Y_val_pred_svm, average='micro'),\n",
    "        f1_score(Y_val, Y_val_pred_perc, average='micro'),\n",
    "        f1_score(Y_val, Y_val_pred_online, average='micro'),\n",
    "        dnn_metrics[\"Micro-F1\"]\n",
    "    ],\n",
    "    \"Macro-F1\": [\n",
    "        f1_score(Y_val, Y_val_pred_lr, average='macro'),\n",
    "        f1_score(Y_val, Y_val_pred_svm, average='macro'),\n",
    "        f1_score(Y_val, Y_val_pred_perc, average='macro'),\n",
    "        f1_score(Y_val, Y_val_pred_online, average='macro'),\n",
    "        dnn_metrics[\"Macro-F1\"]\n",
    "    ],\n",
    "    \"Performance@k\": [\n",
    "        precision_score(Y_val, Y_val_pred_lr, average='micro', zero_division=0),\n",
    "        precision_score(Y_val, Y_val_pred_svm, average='micro', zero_division=0),\n",
    "        precision_score(Y_val, Y_val_pred_perc, average='micro', zero_division=0),\n",
    "        precision_score(Y_val, Y_val_pred_online, average='micro', zero_division=0),\n",
    "        dnn_metrics[\"Precision@k\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Model Performance Summary ===\")\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
